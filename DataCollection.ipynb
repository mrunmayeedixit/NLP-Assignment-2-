{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your API Key here (or keep None if you don't have)\n",
    "api_key = 'rl_bxreLpJLRynSNRyfZ6sG2Gu3F'\n",
    "# If you don't have an API key, set api_key = None\n",
    "\n",
    "# --- API parameters ---\n",
    "site = 'stackoverflow'\n",
    "tagged = 'nlp'\n",
    "pagesize = 100  # Max allowed\n",
    "base_question_url = \"https://api.stackexchange.com/2.3/questions\"\n",
    "base_answer_url = \"https://api.stackexchange.com/2.3/answers/\"\n",
    "total_posts_needed = 20000\n",
    "\n",
    "# --- Storage ---\n",
    "all_posts = []\n",
    "accepted_answer_ids = []\n",
    "page = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting question data collection...\n",
      "âš¡ No more data available or error occurred.\n",
      "âœ…âœ… Finished collecting 0 questions!\n",
      "\n",
      "ðŸš€ Starting accepted answers collection...\n",
      "âœ…âœ… Finished collecting 0 accepted answers!\n",
      "\n",
      "ðŸ”„ Merging accepted answers into posts...\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸš€ Starting question data collection...\")\n",
    "\n",
    "# Step 1: Pull questions\n",
    "while len(all_posts) < total_posts_needed:\n",
    "    params = {\n",
    "        'order': 'desc',\n",
    "        'sort': 'creation',\n",
    "        'tagged': tagged,\n",
    "        'site': site,\n",
    "        'pagesize': 100,\n",
    "        'page': page,\n",
    "        'filter': '!)Q2B_A7F1D9bS7z6B3mJ0p80'  # Custom filter: includes title, body, tags, accepted_answer_id, view_count, creation_date\n",
    "    }\n",
    "    \n",
    "    if api_key:\n",
    "        params['key'] = api_key\n",
    "    \n",
    "    response = requests.get(base_question_url, params=params)\n",
    "    data = response.json()\n",
    "    \n",
    "    if 'items' not in data or not data['items']:\n",
    "        print(\"âš¡ No more data available or error occurred.\")\n",
    "        break\n",
    "\n",
    "    for item in data['items']:\n",
    "        post = {\n",
    "            'question_id': item.get('question_id', None),\n",
    "            'title': item.get('title', ''),\n",
    "            'body': item.get('body', ''),\n",
    "            'tags': item.get('tags', []),\n",
    "            'view_count': item.get('view_count', 0),\n",
    "            'creation_date': item.get('creation_date', None),\n",
    "            'accepted_answer_id': item.get('accepted_answer_id', None)\n",
    "        }\n",
    "        all_posts.append(post)\n",
    "\n",
    "        if post['accepted_answer_id']:\n",
    "            accepted_answer_ids.append(post['accepted_answer_id'])\n",
    "    \n",
    "    page += 1\n",
    "    time.sleep(1)  # Sleep 1 second to respect API limits\n",
    "    \n",
    "    print(f\"âœ… Collected posts so far: {len(all_posts)}\")\n",
    "\n",
    "print(f\"âœ…âœ… Finished collecting {len(all_posts)} questions!\")\n",
    "\n",
    "# Step 2: Pull accepted answers\n",
    "print(\"\\nðŸš€ Starting accepted answers collection...\")\n",
    "\n",
    "accepted_answers = {}\n",
    "batch_size = 100  # Max IDs per request\n",
    "\n",
    "for i in range(0, len(accepted_answer_ids), batch_size):\n",
    "    batch_ids = accepted_answer_ids[i:i+batch_size]\n",
    "    ids_str = ';'.join(str(aid) for aid in batch_ids)\n",
    "    \n",
    "    answer_params = {\n",
    "        'order': 'desc',\n",
    "        'sort': 'activity',\n",
    "        'site': site,\n",
    "        'filter': '!9_bDE(B6I'  # Custom filter for answers: includes body, creation_date\n",
    "    }\n",
    "    \n",
    "    if api_key:\n",
    "        answer_params['key'] = api_key\n",
    "\n",
    "    answer_response = requests.get(base_answer_url + ids_str, params=answer_params)\n",
    "    answer_data = answer_response.json()\n",
    "    \n",
    "    for answer in answer_data.get('items', []):\n",
    "        accepted_answers[answer['answer_id']] = {\n",
    "            'answer_body': answer.get('body', ''),\n",
    "            'answer_creation_date': answer.get('creation_date', None)\n",
    "        }\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "print(f\"âœ…âœ… Finished collecting {len(accepted_answers)} accepted answers!\")\n",
    "\n",
    "# Step 3: Merge accepted answers into posts\n",
    "print(\"\\nðŸ”„ Merging accepted answers into posts...\")\n",
    "\n",
    "for post in all_posts:\n",
    "    aid = post['accepted_answer_id']\n",
    "    if aid in accepted_answers:\n",
    "        post['accepted_answer_body'] = accepted_answers[aid]['answer_body']\n",
    "        post['accepted_answer_creation_date'] = accepted_answers[aid]['answer_creation_date']\n",
    "    else:\n",
    "        post['accepted_answer_body'] = ''\n",
    "        post['accepted_answer_creation_date'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving to CSV...\n",
      "âœ… All data saved to: C:/Users/Mrunmayee Dixit/Documents/Uni/NLP/Assignments/2/stack_nlp_posts_full.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Save everything to CSV\n",
    "print(\"\\nSaving to CSV...\")\n",
    "\n",
    "# Convert list of posts into a DataFrame\n",
    "df = pd.DataFrame(all_posts)\n",
    "\n",
    "# Convert 'tags' list into a string\n",
    "df['tags'] = df['tags'].apply(lambda x: ', '.join(x) if isinstance(x, list) else '')\n",
    "\n",
    "# Save to CSV\n",
    "output_filename = 'C:/Users/Mrunmayee Dixit/Documents/Uni/NLP/Assignments/2/stack_nlp_posts_full.csv'\n",
    "df.to_csv(output_filename, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"âœ… All data saved to: {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Save everything to CSV\n",
    "print(\"\\nSaving to CSV...\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(all_posts)\n",
    "\n",
    "# Convert 'tags' list into a string\n",
    "df['tags'] = df['tags'].apply(lambda x: ', '.join(x) if isinstance(x, list) else '')\n",
    "\n",
    "# Save\n",
    "output_filename = 'C:/Users/Mrunmayee Dixit/Documents/Uni/NLP/Assignments/2/stack_nlp_posts_full_updated.csv'\n",
    "df.to_csv(output_filename, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"ðŸ ALL DONE! Data saved to: {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total questions to fetch: 20079\n",
      "Total accepted answers to fetch: 8193\n",
      "Total questions to fetch: 20079\n",
      "Total accepted answers to fetch: 8193\n",
      "\n",
      "ðŸš€ Fetching updated questions data...\n",
      "âœ… Fetched 0 updated questions.\n",
      "\n",
      "ðŸš€ Fetching updated accepted answers data...\n",
      "âœ… Fetched 0 updated accepted answers.\n",
      "\n",
      "ðŸ”„ Merging updated fields into original DataFrame...\n",
      "\n",
      "ðŸ ALL DONE! Final updated data saved to: C:/Users/Mrunmayee Dixit/Documents/Uni/NLP/Assignments/2/stack_nlp_posts_final.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Your existing data ---\n",
    "file_path = 'C:/Users/Mrunmayee Dixit/Documents/Uni/NLP/Assignments/2/stack_nlp_posts_full.csv'  # <-- Your original 20k CSV\n",
    "api_key = 'rl_PpxUNHis5xSFGzQa6e3UuMDtf'\n",
    "\n",
    "# --- Load the old data ---\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# --- Prepare lists ---\n",
    "# Clean 'question_id' and 'accepted_answer_id' properly\n",
    "df['question_id'] = pd.to_numeric(df['question_id'], errors='coerce')\n",
    "df['accepted_answer_id'] = pd.to_numeric(df['accepted_answer_id'], errors='coerce')\n",
    "\n",
    "# Now safely drop NaNs and convert to int\n",
    "question_ids = df['question_id'].dropna().astype(int).tolist()\n",
    "accepted_answer_ids = df['accepted_answer_id'].dropna().astype(int).tolist()\n",
    "\n",
    "print(f\"Total questions to fetch: {len(question_ids)}\")\n",
    "print(f\"Total accepted answers to fetch: {len(accepted_answer_ids)}\")\n",
    "\n",
    "print(f\"Total questions to fetch: {len(question_ids)}\")\n",
    "print(f\"Total accepted answers to fetch: {len(accepted_answer_ids)}\")\n",
    "\n",
    "# --- API base URLs ---\n",
    "question_api_base = \"https://api.stackexchange.com/2.3/questions/\"\n",
    "answer_api_base = \"https://api.stackexchange.com/2.3/answers/\"\n",
    "\n",
    "# --- Storage ---\n",
    "question_updates = {}\n",
    "answer_updates = {}\n",
    "\n",
    "# --- Fetch questions data ---\n",
    "print(\"\\nðŸš€ Fetching updated questions data...\")\n",
    "batch_size = 100\n",
    "\n",
    "for i in range(0, len(question_ids), batch_size):\n",
    "    batch = question_ids[i:i+batch_size]\n",
    "    ids_str = ';'.join(map(str, batch))\n",
    "    params = {\n",
    "        'order': 'desc',\n",
    "        'sort': 'activity',\n",
    "        'site': 'stackoverflow',\n",
    "        'filter': '!)Q2B_A7F1D9bS7z6B3mJ0p80'  # Pull full question fields\n",
    "    }\n",
    "    if api_key:\n",
    "        params['key'] = api_key\n",
    "    \n",
    "    response = requests.get(question_api_base + ids_str, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    for item in data.get('items', []):\n",
    "        question_updates[item['question_id']] = {\n",
    "            'title': item.get('title', ''),\n",
    "            'body': item.get('body', ''),\n",
    "            'tags': ', '.join(item.get('tags', [])),\n",
    "            'view_count': item.get('view_count', 0),\n",
    "            'creation_date': item.get('creation_date', None),\n",
    "            'accepted_answer_id': item.get('accepted_answer_id', None)\n",
    "        }\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "print(f\"âœ… Fetched {len(question_updates)} updated questions.\")\n",
    "\n",
    "# --- Fetch accepted answers data ---\n",
    "print(\"\\nðŸš€ Fetching updated accepted answers data...\")\n",
    "\n",
    "for i in range(0, len(accepted_answer_ids), batch_size):\n",
    "    batch = accepted_answer_ids[i:i+batch_size]\n",
    "    ids_str = ';'.join(map(str, batch))\n",
    "    params = {\n",
    "        'order': 'desc',\n",
    "        'sort': 'activity',\n",
    "        'site': 'stackoverflow',\n",
    "        'filter': '!9_bDE(B6I'  # Pull full answer fields\n",
    "    }\n",
    "    if api_key:\n",
    "        params['key'] = api_key\n",
    "\n",
    "    response = requests.get(answer_api_base + ids_str, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    for item in data.get('items', []):\n",
    "        answer_updates[item['answer_id']] = {\n",
    "            'accepted_answer_body': item.get('body', ''),\n",
    "            'accepted_answer_creation_date': item.get('creation_date', None)\n",
    "        }\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "print(f\"âœ… Fetched {len(answer_updates)} updated accepted answers.\")\n",
    "\n",
    "# --- Merge back into DataFrame ---\n",
    "print(\"\\nðŸ”„ Merging updated fields into original DataFrame...\")\n",
    "\n",
    "# Update questions\n",
    "for idx, row in df.iterrows():\n",
    "    qid = row['question_id']\n",
    "    if qid in question_updates:\n",
    "        for field in ['title', 'body', 'tags', 'view_count', 'creation_date', 'accepted_answer_id']:\n",
    "            df.at[idx, field] = question_updates[qid][field]\n",
    "\n",
    "# Update answers\n",
    "for idx, row in df.iterrows():\n",
    "    aid = row['accepted_answer_id']\n",
    "    if not pd.isna(aid) and int(aid) in answer_updates:\n",
    "        df.at[idx, 'accepted_answer_body'] = answer_updates[int(aid)]['accepted_answer_body']\n",
    "        df.at[idx, 'accepted_answer_creation_date'] = answer_updates[int(aid)]['accepted_answer_creation_date']\n",
    "\n",
    "# --- Save updated file ---\n",
    "output_file = 'C:/Users/Mrunmayee Dixit/Documents/Uni/NLP/Assignments/2/stack_nlp_posts_final.csv'\n",
    "df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"\\nðŸ ALL DONE! Final updated data saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
